{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bz7A-XhOQ8L4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"data/new_train/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B6bHeDoMQ-y7"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s33LYX3DREUh"
   },
   "outputs": [],
   "source": [
    "batch_sz = 100\n",
    "import numpy as np\n",
    "\n",
    "def my_collate_train(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    batch_inp = []\n",
    "    batch_out = []\n",
    "\n",
    "    for scene in batch:\n",
    "        agent = scene['agent_id']\n",
    "        target = 0\n",
    "        for x in range(len(scene['track_id'])):\n",
    "            if scene['track_id'][x][0] == agent:\n",
    "                target = x\n",
    "        inp = [scene['p_in'][target], scene['v_in'][target]]\n",
    "        out = [scene['p_out'][target], scene['v_out'][target]]\n",
    "        batch_inp.append(inp)\n",
    "        batch_out.append(out) \n",
    "\n",
    "    inp = torch.FloatTensor(batch_inp)\n",
    "    out = torch.FloatTensor(batch_out)\n",
    "    return [inp, out]\n",
    "\n",
    "def my_collate_val(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "\n",
    "    inp = [[scene['p_in'], scene['v_in']] for scene in batch]\n",
    "    mask = [scene['car_mask'] for scene in batch]\n",
    "\n",
    "    inp = torch.LongTensor(inp)\n",
    "    mask = torch.LongTensor(mask)\n",
    "    return [inp, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G7YyLqs1q2_S"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Trajectory(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Trajectory, self).__init__()\n",
    "\n",
    "        self.p_in = nn.Linear(2, 32)\n",
    "        self.v_in = nn.Linear(2, 32)\n",
    "        \n",
    "        self.encoder = nn.LSTM(64, 64, 1)\n",
    "\n",
    "        self.decoder_p = nn.LSTM(64, 128, 1)\n",
    "        self.decoder_v = nn.LSTM(64, 128, 1)\n",
    "\n",
    "        self.p_out = nn.Linear(128, 2)\n",
    "        self.v_out = nn.Linear(128, 2)\n",
    "                \n",
    "    def forward(self, p, v):\n",
    "        batch = p.shape[0]\n",
    "        x_p = self.p_in(p)\n",
    "        x_v = self.v_in(v)\n",
    "\n",
    "        x = torch.cat((x_p, x_v), dim=2)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        _, (state_h, _) = self.encoder(x)\n",
    "\n",
    "        x = state_h.repeat(30, 1, 1)\n",
    "\n",
    "        x_p, _ = self.decoder_p(x)\n",
    "        x_v, _ = self.decoder_v(x)\n",
    "\n",
    "        x_p = x_p.permute(1,0,2)\n",
    "        x_v = x_v.permute(1,0,2)\n",
    "\n",
    "        x_p = self.p_out(x_p)\n",
    "        x_v = self.v_out(x_v)\n",
    "\n",
    "        return x_p, x_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-aPXOGJgnxo",
    "outputId": "443e4c7c-88f4-47f0-fd1d-dffae4086184",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LENGTH OF TRAIN LOADER DATASET: 205942\n",
      "LENGTH OF TRAIN DATA: 164753 \n",
      "LENGTH OF VAL DATA: 41189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/20]: 100%|██████████| 1648/1648 [00:43<00:00, 37.79it/s, loss=17.5]\n",
      "Val.  Epoch [1/20]: 100%|██████████| 412/412 [00:13<00:00, 30.12it/s, loss=1.31] \n",
      "Train Epoch [2/20]: 100%|██████████| 1648/1648 [00:44<00:00, 36.76it/s, loss=6.62]\n",
      "Val.  Epoch [2/20]: 100%|██████████| 412/412 [00:13<00:00, 29.74it/s, loss=1.33] \n",
      "Train Epoch [3/20]: 100%|██████████| 1648/1648 [00:52<00:00, 31.55it/s, loss=5.56]\n",
      "Val.  Epoch [3/20]: 100%|██████████| 412/412 [00:13<00:00, 31.52it/s, loss=1.24] \n",
      "Train Epoch [4/20]: 100%|██████████| 1648/1648 [01:01<00:00, 26.82it/s, loss=5.16]\n",
      "Val.  Epoch [4/20]: 100%|██████████| 412/412 [00:12<00:00, 33.18it/s, loss=1.02] \n",
      "Train Epoch [5/20]: 100%|██████████| 1648/1648 [00:51<00:00, 31.94it/s, loss=4.63]\n",
      "Val.  Epoch [5/20]: 100%|██████████| 412/412 [00:14<00:00, 28.57it/s, loss=0.923]\n",
      "Train Epoch [6/20]: 100%|██████████| 1648/1648 [00:47<00:00, 34.37it/s, loss=4.33]\n",
      "Val.  Epoch [6/20]: 100%|██████████| 412/412 [00:12<00:00, 33.56it/s, loss=1.37] \n",
      "Train Epoch [7/20]: 100%|██████████| 1648/1648 [01:00<00:00, 27.04it/s, loss=4.11]\n",
      "Val.  Epoch [7/20]: 100%|██████████| 412/412 [00:11<00:00, 35.17it/s, loss=0.894]\n",
      "Train Epoch [8/20]: 100%|██████████| 1648/1648 [00:52<00:00, 31.35it/s, loss=3.76]\n",
      "Val.  Epoch [8/20]: 100%|██████████| 412/412 [00:15<00:00, 27.23it/s, loss=1.39] \n",
      "Train Epoch [9/20]: 100%|██████████| 1648/1648 [00:51<00:00, 32.22it/s, loss=3.59]\n",
      "Val.  Epoch [9/20]: 100%|██████████| 412/412 [00:13<00:00, 31.33it/s, loss=0.88] \n",
      "Train Epoch [10/20]: 100%|██████████| 1648/1648 [01:00<00:00, 27.12it/s, loss=3.31]\n",
      "Val.  Epoch [10/20]: 100%|██████████| 412/412 [00:11<00:00, 35.87it/s, loss=1.03] \n",
      "Train Epoch [11/20]: 100%|██████████| 1648/1648 [00:51<00:00, 32.26it/s, loss=3.34]\n",
      "Val.  Epoch [11/20]: 100%|██████████| 412/412 [00:15<00:00, 27.39it/s, loss=0.692]\n",
      "Train Epoch [12/20]: 100%|██████████| 1648/1648 [00:51<00:00, 31.98it/s, loss=3.19]\n",
      "Val.  Epoch [12/20]: 100%|██████████| 412/412 [00:12<00:00, 31.71it/s, loss=0.93] \n",
      "Train Epoch [13/20]: 100%|██████████| 1648/1648 [01:02<00:00, 26.21it/s, loss=3.07]\n",
      "Val.  Epoch [13/20]: 100%|██████████| 412/412 [00:10<00:00, 38.55it/s, loss=0.847]\n",
      "Train Epoch [14/20]: 100%|██████████| 1648/1648 [00:49<00:00, 33.21it/s, loss=2.94]\n",
      "Val.  Epoch [14/20]: 100%|██████████| 412/412 [00:14<00:00, 28.66it/s, loss=0.974]\n",
      "Train Epoch [15/20]: 100%|██████████| 1648/1648 [00:48<00:00, 33.91it/s, loss=2.85]\n",
      "Val.  Epoch [15/20]: 100%|██████████| 412/412 [00:12<00:00, 31.74it/s, loss=0.581]\n",
      "Train Epoch [16/20]: 100%|██████████| 1648/1648 [00:59<00:00, 27.62it/s, loss=2.72]\n",
      "Val.  Epoch [16/20]: 100%|██████████| 412/412 [00:09<00:00, 41.54it/s, loss=0.654]\n",
      "Train Epoch [17/20]: 100%|██████████| 1648/1648 [00:49<00:00, 32.96it/s, loss=2.77]\n",
      "Val.  Epoch [17/20]: 100%|██████████| 412/412 [00:14<00:00, 29.22it/s, loss=0.589]\n",
      "Train Epoch [18/20]: 100%|██████████| 1648/1648 [00:47<00:00, 34.64it/s, loss=2.63]\n",
      "Val.  Epoch [18/20]: 100%|██████████| 412/412 [00:13<00:00, 31.35it/s, loss=0.873]\n",
      "Train Epoch [19/20]: 100%|██████████| 1648/1648 [00:59<00:00, 27.71it/s, loss=2.61]\n",
      "Val.  Epoch [19/20]: 100%|██████████| 412/412 [00:08<00:00, 47.20it/s, loss=0.345] \n",
      "Train Epoch [20/20]: 100%|██████████| 1648/1648 [00:50<00:00, 32.44it/s, loss=2.48]\n",
      "Val.  Epoch [20/20]: 100%|██████████| 412/412 [00:14<00:00, 28.64it/s, loss=0.765]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Single-Car Training\n",
    "Normalization: /4800, /100\n",
    "'''\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, \n",
    "                          shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "val_size = len(train_loader.dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
    "\n",
    "train_data = DataLoader(train_data, batch_size=batch_sz, \n",
    "                        shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "val_data = DataLoader(val_data, batch_size=batch_sz, \n",
    "                      shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
    "    \n",
    "    for i_batch, sample_batch in train_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "        p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        v_criteria = nn.MSELoss()\n",
    "        v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "        loss = p_loss + v_loss\n",
    "        epoch_loss += p_loss\n",
    "\n",
    "        my_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        my_optim.step()\n",
    "    \n",
    "        # update progress bar\n",
    "        train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "        train_loop.set_postfix(loss = epoch_loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
    "    \n",
    "    for i_batch, sample_batch in val_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "        p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. \n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        epoch_loss += p_loss\n",
    "        \n",
    "        val_loop.set_description(f\"Val.  Epoch [{i + 1}/{epoch}]\")\n",
    "        val_loop.set_postfix(loss = epoch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LENGTH OF TRAIN LOADER DATASET: 205942\n",
      "LENGTH OF TRAIN DATA: 164753 \n",
      "LENGTH OF VAL DATA: 41189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/20]: 100%|██████████| 1648/1648 [00:37<00:00, 44.47it/s, loss=22.4]\n",
      "Val.  Epoch [1/20]: 100%|██████████| 412/412 [00:11<00:00, 35.50it/s, loss=1.88] \n",
      "Train Epoch [2/20]: 100%|██████████| 1648/1648 [00:42<00:00, 38.39it/s, loss=8.36]\n",
      "Val.  Epoch [2/20]: 100%|██████████| 412/412 [00:12<00:00, 34.01it/s, loss=2.05] \n",
      "Train Epoch [3/20]: 100%|██████████| 1648/1648 [00:46<00:00, 35.76it/s, loss=7.35]\n",
      "Val.  Epoch [3/20]: 100%|██████████| 412/412 [00:11<00:00, 36.24it/s, loss=1.7]  \n",
      "Train Epoch [4/20]: 100%|██████████| 1648/1648 [00:49<00:00, 33.58it/s, loss=6.61]\n",
      "Val.  Epoch [4/20]: 100%|██████████| 412/412 [00:14<00:00, 29.24it/s, loss=1.8]  \n",
      "Train Epoch [5/20]: 100%|██████████| 1648/1648 [00:54<00:00, 30.38it/s, loss=6.15]\n",
      "Val.  Epoch [5/20]: 100%|██████████| 412/412 [00:13<00:00, 31.42it/s, loss=1.87] \n",
      "Train Epoch [6/20]: 100%|██████████| 1648/1648 [00:59<00:00, 27.77it/s, loss=5.8] \n",
      "Val.  Epoch [6/20]: 100%|██████████| 412/412 [00:10<00:00, 37.48it/s, loss=1.02] \n",
      "Train Epoch [7/20]: 100%|██████████| 1648/1648 [00:56<00:00, 29.24it/s, loss=5.42]\n",
      "Val.  Epoch [7/20]: 100%|██████████| 412/412 [00:15<00:00, 27.46it/s, loss=1.04] \n",
      "Train Epoch [8/20]: 100%|██████████| 1648/1648 [00:52<00:00, 31.20it/s, loss=5.21]\n",
      "Val.  Epoch [8/20]: 100%|██████████| 412/412 [00:13<00:00, 30.30it/s, loss=1.21] \n",
      "Train Epoch [9/20]: 100%|██████████| 1648/1648 [01:00<00:00, 27.24it/s, loss=5]   \n",
      "Val.  Epoch [9/20]: 100%|██████████| 412/412 [00:10<00:00, 39.59it/s, loss=1.13] \n",
      "Train Epoch [10/20]: 100%|██████████| 1648/1648 [00:58<00:00, 28.32it/s, loss=4.8] \n",
      "Val.  Epoch [10/20]: 100%|██████████| 412/412 [00:15<00:00, 26.54it/s, loss=0.871]\n",
      "Train Epoch [11/20]: 100%|██████████| 1648/1648 [00:52<00:00, 31.37it/s, loss=4.71]\n",
      "Val.  Epoch [11/20]: 100%|██████████| 412/412 [00:15<00:00, 26.54it/s, loss=0.894]\n",
      "Train Epoch [12/20]: 100%|██████████| 1648/1648 [01:01<00:00, 26.89it/s, loss=4.53]\n",
      "Val.  Epoch [12/20]: 100%|██████████| 412/412 [00:08<00:00, 45.82it/s, loss=1.62] \n",
      "Train Epoch [13/20]: 100%|██████████| 1648/1648 [00:57<00:00, 28.66it/s, loss=4.37]\n",
      "Val.  Epoch [13/20]: 100%|██████████| 412/412 [00:15<00:00, 25.96it/s, loss=0.909]\n",
      "Train Epoch [14/20]: 100%|██████████| 1648/1648 [00:49<00:00, 33.18it/s, loss=4.24]\n",
      "Val.  Epoch [14/20]: 100%|██████████| 412/412 [00:15<00:00, 26.65it/s, loss=0.964]\n",
      "Train Epoch [15/20]: 100%|██████████| 1648/1648 [01:01<00:00, 26.75it/s, loss=4.07]\n",
      "Val.  Epoch [15/20]: 100%|██████████| 412/412 [00:09<00:00, 45.73it/s, loss=1.1]  \n",
      "Train Epoch [16/20]: 100%|██████████| 1648/1648 [00:56<00:00, 29.27it/s, loss=3.95]\n",
      "Val.  Epoch [16/20]: 100%|██████████| 412/412 [00:15<00:00, 25.95it/s, loss=1.11] \n",
      "Train Epoch [17/20]: 100%|██████████| 1648/1648 [00:50<00:00, 32.94it/s, loss=3.95]\n",
      "Val.  Epoch [17/20]: 100%|██████████| 412/412 [00:15<00:00, 26.37it/s, loss=1.1]  \n",
      "Train Epoch [18/20]: 100%|██████████| 1648/1648 [01:01<00:00, 26.62it/s, loss=3.88]\n",
      "Val.  Epoch [18/20]: 100%|██████████| 412/412 [00:09<00:00, 44.81it/s, loss=0.997]\n",
      "Train Epoch [19/20]: 100%|██████████| 1648/1648 [00:56<00:00, 28.99it/s, loss=3.72]\n",
      "Val.  Epoch [19/20]: 100%|██████████| 412/412 [00:16<00:00, 25.45it/s, loss=0.943]\n",
      "Train Epoch [20/20]: 100%|██████████| 1648/1648 [00:50<00:00, 32.83it/s, loss=3.66]\n",
      "Val.  Epoch [20/20]: 100%|██████████| 412/412 [00:15<00:00, 26.11it/s, loss=0.994]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Single-Car Training\n",
    "Normalization: /2400, /85\n",
    "'''\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, \n",
    "                          shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "val_size = len(train_loader.dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
    "\n",
    "train_data = DataLoader(train_data, batch_size=batch_sz, \n",
    "                        shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "val_data = DataLoader(val_data, batch_size=batch_sz, \n",
    "                      shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
    "    \n",
    "    for i_batch, sample_batch in train_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = ((inp[:,0].reshape(batch*number,19,2).to(device)) - 2400.) / 2400.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/85.\n",
    "        p_out = ((out[:,0].reshape(batch*number,30,2).to(device)) - 2400.) / 2400.\n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/85.\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        v_criteria = nn.MSELoss()\n",
    "        v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "        loss = p_loss + v_loss\n",
    "        epoch_loss += p_loss\n",
    "\n",
    "        my_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        my_optim.step()\n",
    "    \n",
    "        # update progress bar\n",
    "        train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "        train_loop.set_postfix(loss = epoch_loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
    "    \n",
    "    for i_batch, sample_batch in val_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = ((inp[:,0].reshape(batch*number,19,2).to(device)) - 2400.)/2400.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/85.\n",
    "        p_out = ((out[:,0].reshape(batch*number,30,2).to(device)) - 2400.)/2400. \n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/85.\n",
    "\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        epoch_loss += p_loss\n",
    "        \n",
    "        val_loop.set_description(f\"Val.  Epoch [{i + 1}/{epoch}]\")\n",
    "        val_loop.set_postfix(loss = epoch_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Loss = 2.52\n",
    "- Validation Loss = 0.762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': my_optim.state_dict(),\n",
    "}\n",
    "torch.save(state, \"saved_model/PogNet1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LENGTH OF TRAIN LOADER DATASET: 205942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/20]: 100%|██████████| 2060/2060 [01:02<00:00, 33.08it/s, loss=21.5]\n",
      "Train Epoch [2/20]: 100%|██████████| 2060/2060 [01:02<00:00, 32.96it/s, loss=8.61]\n",
      "Train Epoch [3/20]: 100%|██████████| 2060/2060 [00:56<00:00, 36.77it/s, loss=7.25]\n",
      "Train Epoch [4/20]: 100%|██████████| 2060/2060 [00:46<00:00, 44.63it/s, loss=6.51]\n",
      "Train Epoch [5/20]: 100%|██████████| 2060/2060 [00:48<00:00, 42.36it/s, loss=5.83]\n",
      "Train Epoch [6/20]: 100%|██████████| 2060/2060 [00:47<00:00, 43.29it/s, loss=5.34]\n",
      "Train Epoch [7/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.55it/s, loss=4.85]\n",
      "Train Epoch [8/20]: 100%|██████████| 2060/2060 [00:44<00:00, 46.15it/s, loss=4.68]\n",
      "Train Epoch [9/20]: 100%|██████████| 2060/2060 [00:44<00:00, 46.50it/s, loss=4.42]\n",
      "Train Epoch [10/20]: 100%|██████████| 2060/2060 [00:44<00:00, 46.53it/s, loss=4.18]\n",
      "Train Epoch [11/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.41it/s, loss=3.89]\n",
      "Train Epoch [12/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.32it/s, loss=3.79]\n",
      "Train Epoch [13/20]: 100%|██████████| 2060/2060 [00:44<00:00, 46.20it/s, loss=3.63]\n",
      "Train Epoch [14/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.37it/s, loss=3.41]\n",
      "Train Epoch [15/20]: 100%|██████████| 2060/2060 [00:44<00:00, 45.93it/s, loss=3.41]\n",
      "Train Epoch [16/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.43it/s, loss=3.17]\n",
      "Train Epoch [17/20]: 100%|██████████| 2060/2060 [00:45<00:00, 44.99it/s, loss=3.15]\n",
      "Train Epoch [18/20]: 100%|██████████| 2060/2060 [00:45<00:00, 45.69it/s, loss=2.98]\n",
      "Train Epoch [19/20]: 100%|██████████| 2060/2060 [00:45<00:00, 44.87it/s, loss=2.99]\n",
      "Train Epoch [20/20]: 100%|██████████| 2060/2060 [00:45<00:00, 44.81it/s, loss=2.89]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, \n",
    "                          shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    \n",
    "    for i_batch, sample_batch in train_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "        p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        v_criteria = nn.MSELoss()\n",
    "        v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "        loss = p_loss + v_loss\n",
    "        epoch_loss += p_loss\n",
    "\n",
    "        my_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        my_optim.step()\n",
    "    \n",
    "        # update progress bar\n",
    "        train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "        train_loop.set_postfix(loss = epoch_loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uQ2Y3O0ELNN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "fig, (plt1, plt2, plt3) = plt.subplots(3)\n",
    "\n",
    "for i_batch, sample_batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "    \n",
    "    x = pred[0][10]*4800\n",
    "    y = p_out[10]*4800\n",
    "    z = torch.subtract(x, y)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(torch.sum(z))\n",
    "    \n",
    "    plt1.plot(x.cpu().detach().numpy(), y.cpu().numpy())\n",
    "    plt2.plot(y.cpu().numpy())\n",
    "    plt3.plot(z.cpu().detach().numpy())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize Trajectories\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a random agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    \n",
    "    p_in = (inp[:,0].reshape(batch_sz * number, 19, 2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch_sz * number, 19, 2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch_sz * number, 30, 2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch_sz * number, 30, 2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "    print(pred[0].shape)\n",
    "    x = (pred[0][10]*4800).cpu().detach().numpy()\n",
    "    y = (p_out[10]*4800).cpu().numpy()\n",
    "    exit()    \n",
    "    fig, axs = plt.subplots(10, 10, figsize=(50, 50), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1], c=\"blue\")\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1], c=\"red\")\n",
    "        axs[i].scatter(x, y, c=\"black\")\n",
    "\n",
    "model.eval()\n",
    "agent_id = 0\n",
    "     \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QhL6nFx1q3bm"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "model.eval()\n",
    "temp = []\n",
    "\n",
    "new_path = \"data/new_val_in/\"\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "\n",
    "top = []\n",
    "top.append(\"ID\")\n",
    "for i in range(60):\n",
    "    top.append(\"v\"+str(i+1))\n",
    "temp.append(top)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in val_dataset:\n",
    "        row = []\n",
    "        scene = i['scene_idx']\n",
    "        agent = i['agent_id']\n",
    "        target =0\n",
    "        for x in range(len(i['track_id'])):\n",
    "            if i['track_id'][x][0] == agent:\n",
    "                target = x\n",
    "\n",
    "        p_in = torch.LongTensor(i['p_in'])\n",
    "        v_in = torch.LongTensor(i['v_in'])\n",
    "\n",
    "\n",
    "\n",
    "        p_in = (p_in.reshape(60,19,2).to(device))/4800.\n",
    "        v_in = v_in.reshape(60,19,2).to(device)/100.\n",
    "\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        pred_out = pred[0]*4800.\n",
    "      \n",
    "        output = pred_out[target]\n",
    "\n",
    "        row.append(scene)\n",
    "        row = row + torch.flatten(output).cpu().detach().numpy().tolist()\n",
    "        temp.append(row)\n",
    "\n",
    "with open('submission9.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "151B_Kaggle_Comp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
