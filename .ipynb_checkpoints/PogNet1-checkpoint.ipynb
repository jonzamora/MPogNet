{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bz7A-XhOQ8L4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"data/new_train/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B6bHeDoMQ-y7"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s33LYX3DREUh"
   },
   "outputs": [],
   "source": [
    "batch_sz = 100\n",
    "import numpy as np\n",
    "\n",
    "def my_collate_train(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    batch_inp = []\n",
    "    batch_out = []\n",
    "\n",
    "    for scene in batch:\n",
    "        agent = scene['agent_id']\n",
    "        target = 0\n",
    "        for x in range(len(scene['track_id'])):\n",
    "            if scene['track_id'][x][0] == agent:\n",
    "                target = x\n",
    "        inp = [scene['p_in'][target], scene['v_in'][target]]\n",
    "        out = [scene['p_out'][target], scene['v_out'][target]]\n",
    "        batch_inp.append(inp)\n",
    "        batch_out.append(out)\n",
    "\n",
    "    # scene level #####################\n",
    "    # batch_inp = []\n",
    "    # batch_out = []\n",
    "    # for scene in batch:\n",
    "    #   mask = scene['car_mask'].flatten()==1\n",
    "    #   # print(np.count_nonzero(mask))\n",
    "    #   inp = [scene['p_in'][mask], scene['v_in'][mask]]\n",
    "    #   out = [scene['p_out'][mask], scene['v_out'][mask]]\n",
    "    #   batch_inp.append(inp)\n",
    "    #   batch_out.append(out)\n",
    "    ####################################\n",
    "    \n",
    "\n",
    "    inp = torch.FloatTensor(batch_inp)\n",
    "    out = torch.FloatTensor(batch_out)\n",
    "    return [inp, out]\n",
    "\n",
    "def my_collate_val(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "\n",
    "    inp = [[scene['p_in'], scene['v_in']] for scene in batch]\n",
    "    mask = [scene['car_mask'] for scene in batch]\n",
    "\n",
    "    inp = torch.LongTensor(inp)\n",
    "    mask = torch.LongTensor(mask)\n",
    "    return [inp, mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G7YyLqs1q2_S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonzamora/miniconda3/envs/torch/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Trajectory(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Trajectory, self).__init__()\n",
    "\n",
    "        self.p_in = nn.Linear(2,32) # N, 19, 2\n",
    "        self.v_in = nn.Linear(2,32) # N, 19, 2\n",
    "        \n",
    "        self.encoder = nn.LSTM(64, 64, 1) # input 19, N, 64 output 1, N, 64 \n",
    "\n",
    "        self.decoder_p = nn.LSTM(64, 128, 1) # input 30, N, 64 output 30, N, 128\n",
    "        self.decoder_v = nn.LSTM(64, 128,1) # input 30, N, 64 output 30, N, 128\n",
    "\n",
    "        self.p_out = nn.Linear(128,2)\n",
    "        self.v_out = nn.Linear(128,2)\n",
    "\n",
    "    def forward(self, p, v):\n",
    "        batch = p.shape[0]\n",
    "        x_p = self.p_in(p)\n",
    "        x_v = self.v_in(v)\n",
    "\n",
    "        x = torch.cat((x_p,x_v),dim=2)\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        _,(state_h,_) = self.encoder(x)\n",
    "\n",
    "        x = state_h.repeat(30,1,1)     \n",
    "\n",
    "        x_p,_ = self.decoder_p(x)\n",
    "        x_v,_ = self.decoder_v(x)\n",
    "\n",
    "        x_p = x_p.permute(1,0,2)\n",
    "        x_v = x_v.permute(1,0,2)\n",
    "\n",
    "        x_p = self.p_out(x_p)\n",
    "        x_v = self.v_out(x_v)\n",
    "\n",
    "        # batch 50, 345s per epoch\n",
    "        # p to p -> 9.21 after 10 epochs\n",
    "        # p + v to p -> 8.8 after 10 epochs\n",
    "        # p + v to p +v -> 6.65 after 10 epochs\n",
    "\n",
    "        # batch 100, 345s per epoch\n",
    "        # p + v to p +v -> 4.5 after 10 epochs\n",
    "\n",
    "        return x_p , x_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-aPXOGJgnxo",
    "outputId": "443e4c7c-88f4-47f0-fd1d-dffae4086184",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LENGTH OF TRAIN LOADER DATASET: 205942\n",
      "LENGTH OF TRAIN DATA: 164753 \n",
      "LENGTH OF VAL DATA: 41189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/20]:  24%|██▍       | 393/1648 [00:33<01:48, 11.61it/s, loss=12.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c6ae1fdafbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, \n",
    "                          shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "val_size = len(train_loader.dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
    "\n",
    "train_data = DataLoader(train_data, batch_size=batch_sz, \n",
    "                        shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "val_data = DataLoader(val_data, batch_size=batch_sz, \n",
    "                      shuffle=False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
    "    \n",
    "    for i_batch, sample_batch in train_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "        p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        v_criteria = nn.MSELoss()\n",
    "        v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "        loss = p_loss + v_loss\n",
    "        epoch_loss += p_loss\n",
    "\n",
    "        my_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        my_optim.step()\n",
    "    \n",
    "        # update progress bar\n",
    "        train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "        train_loop.set_postfix(loss = epoch_loss.item())\n",
    "\n",
    "        #print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
    "    \n",
    "    for i_batch, sample_batch in val_loop:\n",
    "        inp, out = sample_batch\n",
    "        batch = inp.shape[0]\n",
    "\n",
    "        p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800.\n",
    "        v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "        p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. \n",
    "        v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "        pred = model(p_in, v_in)\n",
    "\n",
    "        loss = 0\n",
    "        p_criteria = nn.MSELoss()\n",
    "        p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "        epoch_loss += p_loss\n",
    "        \n",
    "        val_loop.set_description(f\"Val. Epoch [{i + 1}/{epoch}]\")\n",
    "        val_loop.set_postfix(loss = epoch_loss.item())\n",
    "        \n",
    "    # print(\"Validation Loss: \", epoch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjNAZOPL9cx2",
    "outputId": "c8bf9612-5b09-4ff9-8e41-05d19379edea"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# decayRate = 0.999\n",
    "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  start = time.time()\n",
    "  for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    v_criteria = nn.MSELoss()\n",
    "    v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "    loss = p_loss + v_loss\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    my_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optim.step()\n",
    "    # my_lr_scheduler.step()\n",
    "    # if(i>2999):\n",
    "    #   # print(pred[0]*2400.+2400., p_out[0]*2400.+2400.)\n",
    "    #   print(pred[0]*4800., p_out[0]*4800.)\n",
    "    # break\n",
    "  print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
    "  \n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-uQ2Y3O0ELNN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3855.3159, 2220.7620],\n",
      "        [3852.3689, 2218.5303],\n",
      "        [3854.8416, 2220.0542],\n",
      "        [3853.5981, 2219.4614],\n",
      "        [3853.8918, 2219.7026],\n",
      "        [3854.1421, 2220.0884],\n",
      "        [3854.0605, 2220.4563],\n",
      "        [3854.2258, 2220.6936],\n",
      "        [3854.8645, 2221.0527],\n",
      "        [3854.9539, 2221.0481],\n",
      "        [3855.3323, 2221.3057],\n",
      "        [3856.0032, 2221.7812],\n",
      "        [3856.4998, 2221.8364],\n",
      "        [3857.1455, 2222.1660],\n",
      "        [3857.7339, 2222.4341],\n",
      "        [3857.9233, 2222.5198],\n",
      "        [3858.6946, 2223.0537],\n",
      "        [3859.0073, 2223.0049],\n",
      "        [3859.1829, 2223.1497],\n",
      "        [3860.1641, 2223.6965],\n",
      "        [3860.0190, 2223.7615],\n",
      "        [3860.8083, 2224.0120],\n",
      "        [3861.3203, 2224.6235],\n",
      "        [3862.1252, 2225.0232],\n",
      "        [3861.7603, 2225.0791],\n",
      "        [3862.4116, 2225.5283],\n",
      "        [3862.9006, 2225.8247],\n",
      "        [3863.2871, 2226.1689],\n",
      "        [3863.8145, 2226.5725],\n",
      "        [3863.6619, 2226.7927]], device='cuda:0', grad_fn=<MulBackward0>) tensor([[3857.6289, 2234.0903],\n",
      "        [3857.6509, 2233.8599],\n",
      "        [3857.7341, 2234.0071],\n",
      "        [3858.8374, 2234.7432],\n",
      "        [3858.9382, 2234.5264],\n",
      "        [3859.5488, 2235.1284],\n",
      "        [3859.9846, 2235.3674],\n",
      "        [3860.2905, 2235.2473],\n",
      "        [3860.8152, 2235.6624],\n",
      "        [3861.1067, 2235.7766],\n",
      "        [3861.5142, 2235.7927],\n",
      "        [3862.1494, 2236.2151],\n",
      "        [3863.0422, 2236.5674],\n",
      "        [3863.5269, 2236.7515],\n",
      "        [3863.9468, 2236.7896],\n",
      "        [3863.7832, 2236.7734],\n",
      "        [3864.7966, 2237.2300],\n",
      "        [3865.6831, 2237.3176],\n",
      "        [3866.5605, 2237.6309],\n",
      "        [3866.8245, 2237.7632],\n",
      "        [3866.1465, 2237.6382],\n",
      "        [3866.4241, 2237.8069],\n",
      "        [3866.8464, 2237.9971],\n",
      "        [3867.3887, 2238.2036],\n",
      "        [3868.1062, 2238.2737],\n",
      "        [3868.5491, 2238.3997],\n",
      "        [3868.9929, 2238.5298],\n",
      "        [3869.4226, 2238.6721],\n",
      "        [3870.3257, 2238.9238],\n",
      "        [3870.7759, 2239.0312]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "    print(pred[0][10]*4800, p_out[10]*4800)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhL6nFx1q3bm"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "model.eval()\n",
    "temp = []\n",
    "\n",
    "new_path = \"/content/new_val_in\"\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "\n",
    "top = []\n",
    "top.append(\"ID\")\n",
    "for i in range(60):\n",
    "  top.append(\"v\"+str(i+1))\n",
    "temp.append(top)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in val_dataset:\n",
    "      row = []\n",
    "      scene = i['scene_idx']\n",
    "      agent = i['agent_id']\n",
    "      target =0\n",
    "      for x in range(len(i['track_id'])):\n",
    "        if i['track_id'][x][0] == agent:\n",
    "          target = x\n",
    "\n",
    "      p_in = torch.LongTensor(i['p_in'])\n",
    "      v_in = torch.LongTensor(i['v_in'])\n",
    "\n",
    "\n",
    "\n",
    "      p_in = (p_in.reshape(60,19,2).to(device))/4800.\n",
    "      v_in = v_in.reshape(60,19,2).to(device)/100.\n",
    "\n",
    "      pred = model(p_in, v_in)\n",
    "\n",
    "      pred_out = pred[0]*4800.\n",
    "      \n",
    "      output = pred_out[target]\n",
    "\n",
    "      row.append(scene)\n",
    "      row = row + torch.flatten(output).cpu().detach().numpy().tolist()\n",
    "      temp.append(row)\n",
    "\n",
    "with open('submission2.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(temp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "151B_Kaggle_Comp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
