{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PogNet3\n",
    "\n",
    "Multi-Vehicle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bz7A-XhOQ8L4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os, os.path \n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"/content/new_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B6bHeDoMQ-y7"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s33LYX3DREUh"
   },
   "outputs": [],
   "source": [
    "batch_sz = 100\n",
    "\n",
    "def my_collate_train(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "\n",
    "    batch_inp = []\n",
    "    batch_out = []\n",
    "\n",
    "    for scene in batch:\n",
    "      agent = scene['agent_id']\n",
    "      target = 0\n",
    "      for x in range(len(scene['track_id'])):\n",
    "        if scene['track_id'][x][0] == agent:\n",
    "          target = x\n",
    "      inp = [scene['p_in'][target], scene['v_in'][target]]\n",
    "      out = [scene['p_out'][target], scene['v_out'][target]]\n",
    "      batch_inp.append(inp)\n",
    "      batch_out.append(out)\n",
    "\n",
    "    # scene level #####################\n",
    "    # batch_inp = []\n",
    "    # batch_out = []\n",
    "    # for scene in batch:\n",
    "    #   mask = scene['car_mask'].flatten()==1\n",
    "    #   # print(np.count_nonzero(mask))\n",
    "    #   inp = [scene['p_in'][mask], scene['v_in'][mask]]\n",
    "    #   out = [scene['p_out'][mask], scene['v_out'][mask]]\n",
    "    #   batch_inp.append(inp)\n",
    "    #   batch_out.append(out)\n",
    "    ####################################\n",
    "\n",
    "\n",
    "    inp = torch.LongTensor(batch_inp)\n",
    "    out = torch.LongTensor(batch_out)\n",
    "    return [inp, out]\n",
    "\n",
    "def my_collate_train_multiple(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    batch_inp = []\n",
    "    batch_out = []\n",
    "\n",
    "    for scene in batch:\n",
    "      agent = scene['agent_id']\n",
    "      target = 0\n",
    "      for x in range(len(scene['track_id'])):\n",
    "        if scene['track_id'][x][0] == agent:\n",
    "          target = x\n",
    "      inp = [scene['p_in'][target], scene['v_in'][target]]\n",
    "      out = [scene['p_out'][target], scene['v_out'][target]]\n",
    "\n",
    "      other_in = np.zeros((5,2,19,2)) # need to permute to 2,5,19,2 later\n",
    "      other_out = np.zeros((5,2,30,2))\n",
    "\n",
    "      other_in[0] = inp\n",
    "      other_out[0] = out\n",
    "\n",
    "      mask = scene['car_mask'].flatten()==1\n",
    "      mask = np.delete(mask,target)\n",
    "      mask = np.where(mask == True)[0]\n",
    "\n",
    "      if(len(mask)>=4):\n",
    "        temp = random.sample(mask.tolist(),4)\n",
    "        for i in range(len(temp)):\n",
    "          other_in[i+1] = [scene['p_in'][temp[i]], scene['v_in'][temp[i]]]\n",
    "          other_out[i+1] = [scene['p_out'][temp[i]], scene['v_out'][temp[i]]]\n",
    "      else:\n",
    "        for i in range(len(mask)):\n",
    "          other_in[i+1] = [scene['p_in'][mask[i]], scene['v_in'][mask[i]]]\n",
    "          other_out[i+1] = [scene['p_out'][mask[i]], scene['v_out'][mask[i]]]\n",
    "        for i in range(4-len(mask)):\n",
    "          other_in[i+1+len(mask)] = inp\n",
    "          other_out[i+1+len(mask)] = out\n",
    "\n",
    "      batch_inp.append(other_in.tolist())\n",
    "      batch_out.append(other_out.tolist())\n",
    "\n",
    "    # scene level #####################\n",
    "    # batch_inp = []\n",
    "    # batch_out = []\n",
    "    # for scene in batch:\n",
    "    #   mask = scene['car_mask'].flatten()==1\n",
    "    #   # print(np.count_nonzero(mask))\n",
    "    #   inp = [scene['p_in'][mask], scene['v_in'][mask]]\n",
    "    #   out = [scene['p_out'][mask], scene['v_out'][mask]]\n",
    "    #   batch_inp.append(inp)\n",
    "    #   batch_out.append(out)\n",
    "    ####################################\n",
    "\n",
    "\n",
    "    inp = torch.LongTensor(batch_inp)\n",
    "    inp = inp.permute(2,0,1,3,4) # p/v, batch, cars, points, x/y\n",
    "    out = torch.LongTensor(batch_out)\n",
    "    out = out.permute(2,0,1,3,4) \n",
    "    return [inp, out]\n",
    "\n",
    "def my_collate_val(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "\n",
    "    inp = [[scene['p_in'], scene['v_in']] for scene in batch]\n",
    "    mask = [scene['car_mask'] for scene in batch]\n",
    "\n",
    "    inp = torch.LongTensor(inp)\n",
    "    mask = torch.LongTensor(mask)\n",
    "    return [inp, mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elKNcVDyEUOo",
    "outputId": "14274aeb-942b-41b8-fc43-6c397d711927"
   },
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "max_p_x = 0\n",
    "min_p_x = 5000\n",
    "max_p_y = 0\n",
    "min_p_y = 5000\n",
    "max_v_x = 0\n",
    "min_v_x = 100\n",
    "max_v_y = 0\n",
    "min_v_y = 100\n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    for i in inp:\n",
    "      if i[0][:,0].max()> max_p_x:\n",
    "        max_p_x = i[0][:,0].max()\n",
    "      if i[0][:,0].min() < min_p_x:\n",
    "        min_p_x = i[0][:,0].min()\n",
    "      if i[0][:,1].max()> max_p_y:\n",
    "        max_p_y = i[0][:,1].max()\n",
    "      if i[0][:,1].min() < min_p_y:\n",
    "        min_p_y = i[0][:,1].min()\n",
    "\n",
    "      if i[1][:,0].max()> max_v_x:\n",
    "        max_v_x = i[1][:,0].max()\n",
    "      if i[1][:,0].min()< min_v_x:\n",
    "        min_v_x = i[1][:,0].min()\n",
    "      if i[1][:,1].max()> max_v_y:\n",
    "        max_v_y = i[1][:,1].max()\n",
    "      if i[1][:,1].min()< min_v_y:\n",
    "        min_v_y = i[1][:,1].min()\n",
    "\n",
    "    for o in out:\n",
    "      if o[0][:,0].max()> max_p_x:\n",
    "        max_p_x = o[0][:,0].max()\n",
    "      if o[0][:,0].min() < min_p_x:\n",
    "        min_p_x = o[0][:,0].min()\n",
    "      if o[0][:,1].max()> max_p_y:\n",
    "        max_p_y = o[0][:,1].max()\n",
    "      if o[0][:,1].min() < min_p_y:\n",
    "        min_p_y = o[0][:,1].min()\n",
    "\n",
    "      if o[1][:,0].max()> max_v_x:\n",
    "        max_v_x = o[1][:,0].max()\n",
    "      if o[1][:,0].min()< min_v_x:\n",
    "        min_v_x = o[1][:,0].min()\n",
    "      if o[1][:,1].max()> max_v_y:\n",
    "        max_v_y = o[1][:,1].max()\n",
    "      if o[1][:,1].min()< min_v_y:\n",
    "        min_v_y = o[1][:,1].min()\n",
    "\n",
    "    if(i_batch%100==0):\n",
    "      print(i_batch)\n",
    "\n",
    "print(max_p_x, min_p_x, max_p_y, min_p_y) # [4800,-50,4800,-50]\n",
    "print(max_v_x, min_v_x, max_v_y, min_v_y) # [200,-200,200,-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7YyLqs1q2_S"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Edward's Tests\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Trajectory(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Trajectory, self).__init__()\n",
    "\n",
    "        self.p_in = nn.Linear(2,32) # N, 19, 2\n",
    "        self.v_in = nn.Linear(2,32) # N, 19, 2\n",
    "        \n",
    "        self.encoder = nn.LSTM(64, 64, 1) # input 19, N, 64 output 1, N, 64 \n",
    "\n",
    "        self.decoder_p = nn.LSTM(64, 128, 1) # input 30, N, 64 output 30, N, 128\n",
    "        self.decoder_v = nn.LSTM(64, 128,1) # input 30, N, 64 output 30, N, 128\n",
    "\n",
    "        self.p_out = nn.Linear(128,2)\n",
    "        self.v_out = nn.Linear(128,2)\n",
    "\n",
    "    def forward(self, p, v):\n",
    "        batch = p.shape[0]\n",
    "        x_p = self.p_in(p)\n",
    "        x_v = self.v_in(v)\n",
    "\n",
    "        x = torch.cat((x_p,x_v),dim=2)\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        _,(state_h,_) = self.encoder(x)\n",
    "\n",
    "        x = state_h.repeat(30,1,1)     \n",
    "\n",
    "        x_p,_ = self.decoder_p(x)\n",
    "        x_v,_ = self.decoder_v(x)\n",
    "\n",
    "        x_p = x_p.permute(1,0,2)\n",
    "        x_v = x_v.permute(1,0,2)\n",
    "\n",
    "        x_p = self.p_out(x_p)\n",
    "        x_v = self.v_out(x_v)\n",
    "\n",
    "        # batch 50, 345s per epoch\n",
    "        # p to p -> 9.21 after 10 epochs\n",
    "        # p + v to p -> 8.8 after 10 epochs\n",
    "        # p + v to p +v -> 6.65 after 10 epochs\n",
    "\n",
    "        # batch 100, 345s per epoch\n",
    "        # p + v to p +v -> 4.5 after 10 epochs\n",
    "\n",
    "        return x_p , x_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFidR0mWBYc6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Jon's Tests\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Trajectory(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Trajectory, self).__init__()\n",
    "\n",
    "        self.p_in = nn.Linear(2,32) # N, 19, 2\n",
    "        self.v_in = nn.Linear(2,32) # N, 19, 2\n",
    "        \n",
    "        self.encoder = nn.LSTM(64, 64, 1) # input 19, N, 64 output 1, N, 64 \n",
    "\n",
    "        self.decoder_p = nn.LSTM(64, 32, 1) # input 30, N, 64 output 30, N, 128\n",
    "        self.decoder_v = nn.LSTM(64, 32,1) # input 30, N, 64 output 30, N, 128\n",
    "\n",
    "        self.p_out = nn.Linear(32,2)\n",
    "        self.v_out = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, p, v):\n",
    "        batch = p.shape[0]\n",
    "        x_p = self.p_in(p)\n",
    "        x_v = self.v_in(v)\n",
    "\n",
    "        x = torch.cat((x_p,x_v),dim=2)\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        _,(state_h,_) = self.encoder(x)\n",
    "\n",
    "        x = state_h.repeat(30,1,1)     \n",
    "\n",
    "        x_p,_ = self.decoder_p(x)\n",
    "        x_v,_ = self.decoder_v(x)\n",
    "\n",
    "        x_p = x_p.permute(1,0,2)\n",
    "        x_v = x_v.permute(1,0,2)\n",
    "\n",
    "        x_p = self.p_out(x_p)\n",
    "        x_v = self.v_out(x_v)\n",
    "\n",
    "        # batch 50, 345s per epoch\n",
    "        # p to p -> 9.21 after 10 epochs\n",
    "        # p + v to p -> 8.8 after 10 epochs\n",
    "        # p + v to p +v -> 6.65 after 10 epochs\n",
    "\n",
    "        # batch 100, 345s per epoch\n",
    "        # p + v to p +v -> 4.5 after 10 epochs\n",
    "\n",
    "        return x_p , x_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkaCfzwse-L7"
   },
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "import numpy as np\n",
    "model = Trajectory().double()\n",
    "print(model)\n",
    "\n",
    "p = np.zeros((4,19,2))\n",
    "v = np.zeros((4,19,2))\n",
    "p_torch = torch.tensor(p)\n",
    "v_torch = torch.tensor(v)\n",
    "\n",
    "output = model(p_torch,v_torch)\n",
    "print(len(output))\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-aPXOGJgnxo"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "val_size = len(train_loader.dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
    "\n",
    "train_data = DataLoader(train_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "val_data = DataLoader(val_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# decayRate = 0.999    \n",
    "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
    "\n",
    "  for i_batch, sample_batch in train_loop:\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    v_criteria = nn.MSELoss()\n",
    "    v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "    loss = p_loss + v_loss\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    my_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optim.step()\n",
    "\n",
    "    train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "    train_loop.set_postfix(loss = epoch_loss.item())\n",
    "    # my_lr_scheduler.step()\n",
    "    # if(i>2999):\n",
    "    #   # print(pred[0]*2400.+2400., p_out[0]*2400.+2400.)\n",
    "    #   print(pred[0]*4800., p_out[0]*4800.)\n",
    "    # break\n",
    "  #print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
    "  \n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
    "  for i_batch, sample_batch in val_loop:\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. \n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    val_loop.set_description(f\"Val.  Epoch [{i + 1}/{epoch}]\")\n",
    "    val_loop.set_postfix(loss = epoch_loss.item())\n",
    "  #print(\"Validation Loss: \", epoch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wqSU86YvIgT",
    "outputId": "3fc04c5d-5d03-4f1b-e63f-735139370517"
   },
   "outputs": [],
   "source": [
    "# train on multiple cars per\n",
    "import time\n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
    "\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "val_size = len(train_loader.dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
    "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
    "\n",
    "train_data = DataLoader(train_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
    "val_data = DataLoader(val_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# decayRate = 0.999    \n",
    "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 5 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  start = time.time()\n",
    "  \n",
    "  # train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
    "\n",
    "  for i_batch, sample_batch in enumerate(train_data):\n",
    "\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[1]\n",
    "\n",
    "    p_in = ((inp[0].reshape(batch*number,19,2).to(device))-2400.)/2400. # /4800.\n",
    "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
    "    p_out = ((out[0].reshape(batch*number,30,2).to(device))-2400.)/2400. # /4800.\n",
    "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    v_criteria = nn.MSELoss()\n",
    "    v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "    loss = p_loss + v_loss\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    my_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optim.step()\n",
    "\n",
    "    # ur tqdm stuff broke\n",
    "    # train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
    "    # train_loop.set_postfix(loss = epoch_loss.item()) \n",
    "    # my_lr_scheduler.step()\n",
    "    # if(i>2999):\n",
    "    #   # print(pred[0]*2400.+2400., p_out[0]*2400.+2400.)\n",
    "    #   print(pred[0]*4800., p_out[0]*4800.)\n",
    "    # break\n",
    "  print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
    "  \n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  # val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
    "  for i_batch, sample_batch in enumerate(val_data):\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[1]\n",
    "\n",
    "    p_in = ((inp[0].reshape(batch*number,19,2).to(device))-2400.)/2400.\n",
    "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
    "    p_out = ((out[0].reshape(batch*number,30,2).to(device))-2400.)/2400.\n",
    "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    # val_loop.set_description(f\"Val.  Epoch [{i + 1}/{epoch}]\")\n",
    "    # val_loop.set_postfix(loss = epoch_loss.item())\n",
    "  print(\"Validation Loss: \", epoch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjNAZOPL9cx2",
    "outputId": "c8bf9612-5b09-4ff9-8e41-05d19379edea"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "batch_sz = 100\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
    "\n",
    "model = Trajectory().to(device)\n",
    "\n",
    "my_optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# decayRate = 0.999\n",
    "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
    "\n",
    "epoch = 20 # takes around 20 epochs to converge\n",
    "number = 1 # number of cars in each \n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  start = time.time()\n",
    "  for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    batch = inp.shape[0]\n",
    "\n",
    "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "    pred = model(p_in, v_in)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    p_criteria = nn.MSELoss()\n",
    "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
    "\n",
    "    v_criteria = nn.MSELoss()\n",
    "    v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
    "\n",
    "    loss = p_loss + v_loss\n",
    "    epoch_loss += p_loss\n",
    "\n",
    "    my_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optim.step()\n",
    "    # my_lr_scheduler.step()\n",
    "    # if(i>2999):\n",
    "    #   # print(pred[0]*2400.+2400., p_out[0]*2400.+2400.)\n",
    "    #   print(pred[0]*4800., p_out[0]*4800.)\n",
    "    # break\n",
    "  print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
    "  \n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uQ2Y3O0ELNN"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "  inp, out = sample_batch\n",
    "  batch = inp.shape[0]\n",
    "\n",
    "  p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
    "  v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
    "  p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
    "  v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
    "\n",
    "  pred = model(p_in, v_in)\n",
    "  print(pred[0][10]*4800,p_out[10]*4800)\n",
    "  print(torch.subtract(pred[0][10]*4800, p_out[10]*4800))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhL6nFx1q3bm"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "model.eval()\n",
    "temp = []\n",
    "\n",
    "new_path = \"/content/new_val_in\"\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "\n",
    "top = []\n",
    "top.append(\"ID\")\n",
    "for i in range(60):\n",
    "  top.append(\"v\"+str(i+1))\n",
    "temp.append(top)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in val_dataset:\n",
    "      row = []\n",
    "      scene = i['scene_idx']\n",
    "      agent = i['agent_id']\n",
    "      target =0\n",
    "      for x in range(len(i['track_id'])):\n",
    "        if i['track_id'][x][0] == agent:\n",
    "          target = x\n",
    "\n",
    "      p_in = torch.LongTensor(i['p_in'])\n",
    "      v_in = torch.LongTensor(i['v_in'])\n",
    "\n",
    "\n",
    "\n",
    "      p_in = (p_in.reshape(60,19,2).to(device))/4800.\n",
    "      v_in = v_in.reshape(60,19,2).to(device)/100.\n",
    "\n",
    "      pred = model(p_in, v_in)\n",
    "\n",
    "      pred_out = pred[0]*4800.\n",
    "      \n",
    "      output = pred_out[target]\n",
    "\n",
    "      row.append(scene)\n",
    "      row = row + torch.flatten(output).cpu().detach().numpy().tolist()\n",
    "      temp.append(row)\n",
    "\n",
    "with open('submission2.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(temp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "151B_Kaggle_Comp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
