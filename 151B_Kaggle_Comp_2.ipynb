{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "151B_Kaggle_Comp_2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxVcyAsPWKYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b162a0d-874d-4713-c6e1-72124d1056fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7XQ4vP9YwDh"
      },
      "source": [
        "!yes | unzip /content/gdrive/MyDrive/151B_data/new_train.zip > /dev/null\n",
        "!yes | unzip /content/gdrive/MyDrive/151B_data/new_val_in.zip > /dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz7A-XhOQ8L4"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import os, os.path \n",
        "import numpy \n",
        "import pickle\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "\"\"\"Change to the data folder\"\"\"\n",
        "new_path = \"/content/new_train\"\n",
        "\n",
        "# number of sequences in each dataset\n",
        "# train:205942  val:3200 test: 36272 \n",
        "# sequences sampled at 10HZ rate"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bHeDoMQ-y7"
      },
      "source": [
        "class ArgoverseDataset(Dataset):\n",
        "    \"\"\"Dataset class for Argoverse\"\"\"\n",
        "    def __init__(self, data_path: str, transform=None):\n",
        "        super(ArgoverseDataset, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "\n",
        "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
        "        self.pkl_list.sort()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pkl_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        pkl_path = self.pkl_list[idx]\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            \n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "# intialize a dataset\n",
        "train_dataset  = ArgoverseDataset(data_path=new_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s33LYX3DREUh"
      },
      "source": [
        "batch_sz = 100\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def my_collate_train(batch):\n",
        "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
        "\n",
        "    batch_inp = []\n",
        "    batch_out = []\n",
        "\n",
        "    for scene in batch:\n",
        "      agent = scene['agent_id']\n",
        "      target = 0\n",
        "      for x in range(len(scene['track_id'])):\n",
        "        if scene['track_id'][x][0] == agent:\n",
        "          target = x\n",
        "      inp = [scene['p_in'][target], scene['v_in'][target]]\n",
        "      out = [scene['p_out'][target], scene['v_out'][target]]\n",
        "      batch_inp.append(inp)\n",
        "      batch_out.append(out)\n",
        "\n",
        "    # scene level #####################\n",
        "    # batch_inp = []\n",
        "    # batch_out = []\n",
        "    # for scene in batch:\n",
        "    #   mask = scene['car_mask'].flatten()==1\n",
        "    #   # print(np.count_nonzero(mask))\n",
        "    #   inp = [scene['p_in'][mask], scene['v_in'][mask]]\n",
        "    #   out = [scene['p_out'][mask], scene['v_out'][mask]]\n",
        "    #   batch_inp.append(inp)\n",
        "    #   batch_out.append(out)\n",
        "    ####################################\n",
        "\n",
        "\n",
        "    inp = torch.LongTensor(batch_inp)\n",
        "    out = torch.LongTensor(batch_out)\n",
        "    return [inp, out]\n",
        "\n",
        "def my_collate_train_multiple(batch):\n",
        "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
        "\n",
        "    batch_inp = []\n",
        "    batch_out = []\n",
        "\n",
        "    for scene in batch:\n",
        "      agent = scene['agent_id']\n",
        "      target = 0\n",
        "      for x in range(len(scene['track_id'])):\n",
        "        if scene['track_id'][x][0] == agent:\n",
        "          target = x\n",
        "      inp = [scene['p_in'][target], scene['v_in'][target]]\n",
        "      out = [scene['p_out'][target], scene['v_out'][target]]\n",
        "\n",
        "      other_in = np.zeros((5,2,19,2)) # need to permute to 2,5,19,2 later\n",
        "      other_out = np.zeros((5,2,30,2))\n",
        "\n",
        "      other_in[0] = inp\n",
        "      other_out[0] = out\n",
        "\n",
        "      mask = scene['car_mask'].flatten()==1\n",
        "      mask = np.delete(mask,target)\n",
        "      mask = np.where(mask == True)[0]\n",
        "\n",
        "      if(len(mask)>=4):\n",
        "        temp = random.sample(mask.tolist(),4)\n",
        "        for i in range(len(temp)):\n",
        "          other_in[i+1] = [scene['p_in'][temp[i]], scene['v_in'][temp[i]]]\n",
        "          other_out[i+1] = [scene['p_out'][temp[i]], scene['v_out'][temp[i]]]\n",
        "      else:\n",
        "        for i in range(len(mask)):\n",
        "          other_in[i+1] = [scene['p_in'][mask[i]], scene['v_in'][mask[i]]]\n",
        "          other_out[i+1] = [scene['p_out'][mask[i]], scene['v_out'][mask[i]]]\n",
        "        for i in range(4-len(mask)):\n",
        "          other_in[i+1+len(mask)] = inp\n",
        "          other_out[i+1+len(mask)] = out\n",
        "\n",
        "      batch_inp.append(other_in.tolist())\n",
        "      batch_out.append(other_out.tolist())\n",
        "\n",
        "    # scene level #####################\n",
        "    # batch_inp = []\n",
        "    # batch_out = []\n",
        "    # for scene in batch:\n",
        "    #   mask = scene['car_mask'].flatten()==1\n",
        "    #   # print(np.count_nonzero(mask))\n",
        "    #   inp = [scene['p_in'][mask], scene['v_in'][mask]]\n",
        "    #   out = [scene['p_out'][mask], scene['v_out'][mask]]\n",
        "    #   batch_inp.append(inp)\n",
        "    #   batch_out.append(out)\n",
        "    ####################################\n",
        "\n",
        "\n",
        "    inp = torch.LongTensor(batch_inp)\n",
        "    inp = inp.permute(2,0,1,3,4) # p/v, batch, cars, points, x/y\n",
        "    out = torch.LongTensor(batch_out)\n",
        "    out = out.permute(2,0,1,3,4) \n",
        "    return [inp, out]\n",
        "\n",
        "def my_collate_val(batch):\n",
        "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
        "\n",
        "    inp = [[scene['p_in'], scene['v_in']] for scene in batch]\n",
        "    mask = [scene['car_mask'] for scene in batch]\n",
        "\n",
        "    inp = torch.LongTensor(inp)\n",
        "    mask = torch.LongTensor(mask)\n",
        "    return [inp, mask]\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7YyLqs1q2_S"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Trajectory(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Trajectory, self).__init__()\n",
        "\n",
        "        self.p_in = nn.Linear(2,32) # N, 19, 2\n",
        "        self.v_in = nn.Linear(2,32) # N, 19, 2\n",
        "        \n",
        "        self.decoder_p = nn.LSTM(64, 128, 1) # input 19, N, 64 output 1, N, 128\n",
        "        self.decoder_v = nn.LSTM(64, 128,1) # input 19, N, 64 output 1, N, 128\n",
        "\n",
        "        self.p_out = nn.Linear(128,2)\n",
        "        self.v_out = nn.Linear(128,2)\n",
        "\n",
        "    def forward(self, p, v):\n",
        "        batch = p.shape[0]\n",
        "        x_p = self.p_in(p)\n",
        "        x_v = self.v_in(v)\n",
        "\n",
        "        x = torch.cat((x_p,x_v),dim=2)\n",
        "        x = x.permute(1,0,2)\n",
        "\n",
        "        x_p,_ = self.decoder_p(x)\n",
        "        x_v,_ = self.decoder_v(x)\n",
        "\n",
        "        x_p = x_p[-1]\n",
        "        x_v = x_v[-1]\n",
        "      \n",
        "        x_p = self.p_out(x_p)\n",
        "        x_v = self.v_out(x_v)\n",
        "\n",
        "        return x_p , x_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO7RWLpzy4LP"
      },
      "source": [
        "'''\n",
        "Recursive per point\n",
        "'''\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Trajectory(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Trajectory, self).__init__()\n",
        "\n",
        "        self.p_in = nn.Linear(2,32) # N, 19, 2\n",
        "        self.v_in = nn.Linear(2,32) # N, 19, 2\n",
        "        \n",
        "        self.encoder = nn.LSTM(64, 64, 1) # input 19, N, 64 output 1, N, 64 \n",
        "\n",
        "        self.decoder_p = nn.LSTM(64, 64, 1) # input 30, N, 64 output 30, N, 64\n",
        "        self.decoder_v = nn.LSTM(64, 64,1) # input 30, N, 64 output 30, N, 64\n",
        "\n",
        "        self.recurse_p =  nn.Linear(2,32)\n",
        "        self.recurse_v = nn.Linear(2,32)\n",
        " \n",
        "        self.p_out = nn.Linear(64,2)\n",
        "        self.v_out = nn.Linear(64,2)\n",
        "\n",
        "    def forward(self, p, v):\n",
        "        batch = p.shape[0]\n",
        "        x_p = self.p_in(p)\n",
        "        x_v = self.v_in(v)\n",
        "\n",
        "        x = torch.cat((x_p,x_v),dim=2)\n",
        "        x = x.permute(1,0,2)\n",
        "\n",
        "        _,(state_h) = self.encoder(x)\n",
        "\n",
        "        temp_p = torch.zeros(30,batch,2).to(device) # output of p\n",
        "        temp_v = torch.zeros(30,batch,2).to(device)\n",
        "       \n",
        "        decoder_p_inp = torch.unsqueeze(x[-1],0)\n",
        "        decoder_v_inp = torch.unsqueeze(x[-1],0)\n",
        "        state_p = state_h\n",
        "        state_v = state_h\n",
        "\n",
        "        for i in range(30):\n",
        "          x_p,state_p = self.decoder_p(decoder_p_inp, state_p)\n",
        "          x_v,state_v = self.decoder_v(decoder_v_inp, state_v)\n",
        "\n",
        "          x_p = self.p_out(x_p)\n",
        "          x_v = self.v_out(x_v)\n",
        "\n",
        "          temp_p[i] = x_p\n",
        "          temp_v[i] = x_v\n",
        "\n",
        "          x_p = self.recurse_p(x_p)\n",
        "          x_v = self.recurse_v(x_v)\n",
        "\n",
        "          x = torch.cat((x_p,x_v),dim=2)\n",
        "\n",
        "          decoder_p_inp = x\n",
        "          decoder_v_inp = x\n",
        "\n",
        "        temp_p = temp_p.permute(1,0,2)\n",
        "        temp_v = temp_v.permute(1,0,2)\n",
        "        return temp_p, temp_v"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkaCfzwse-L7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa85110-1f73-4f01-84e0-d6a57ce06972"
      },
      "source": [
        "# don't run\n",
        "\n",
        "import numpy as np\n",
        "model = Trajectory().double()\n",
        "print(model)\n",
        "\n",
        "p = np.zeros((4,19,2))\n",
        "v = np.zeros((4,19,2))\n",
        "p_torch = torch.tensor(p)\n",
        "v_torch = torch.tensor(v)\n",
        "\n",
        "output = model(p_torch,v_torch)\n",
        "print(len(output))\n",
        "print(output[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trajectory(\n",
            "  (p_in): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (v_in): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (encoder): LSTM(64, 64)\n",
            "  (decoder_p): LSTM(64, 64)\n",
            "  (decoder_v): LSTM(64, 64)\n",
            "  (recurse_p): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (recurse_v): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (p_out): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (v_out): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "2\n",
            "torch.Size([4, 30, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQfAfZhuk5Z8"
      },
      "source": [
        "def zeroNormNP(array, size=19, pos=np.array([-1,-1])): # array is N,19,2\n",
        "  temp = []\n",
        "  if( np.array_equal(pos, np.array([-1,-1]))):\n",
        "    temp = array[:,0]\n",
        "  else:\n",
        "    temp = pos # should be N,2\n",
        "  temp = np.expand_dims(temp,1)\n",
        "  temp = np.repeat(temp,size,1)\n",
        "\n",
        "  return (array-temp)/200., array[:,0]\n",
        "\n",
        "def zeroNormTor(array, size=19, pos=torch.tensor([-1,-1])): # array is N,19,2\n",
        "  temp = []\n",
        "  if( np.array_equal(pos, torch.tensor([-1,-1]))):\n",
        "    temp = array[:,0]\n",
        "  else:\n",
        "    temp = pos # should be N,2\n",
        "  temp = torch.unsqueeze(temp,1)\n",
        "  temp = torch.repeat_interleave(temp,size,1)\n",
        "  return (array-temp)/200., array[:,0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-aPXOGJgnxo"
      },
      "source": [
        "import time \n",
        "\n",
        "batch_sz = 100\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "\n",
        "train_size = int(0.8 * len(train_loader.dataset))\n",
        "val_size = len(train_loader.dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
        "\n",
        "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
        "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
        "\n",
        "train_data = DataLoader(train_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "val_data = DataLoader(val_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "model = Trajectory().to(device)\n",
        "\n",
        "p_criteria = nn.MSELoss()\n",
        "v_criteria = nn.MSELoss()\n",
        "\n",
        "my_optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# decayRate = 0.999\n",
        "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
        "\n",
        "epoch = 20 # takes around 20 epochs to converge\n",
        "number = 5 # number of cars in each \n",
        "\n",
        "for t in range(epoch):\n",
        "\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  start = time.time()\n",
        "  for i_batch, sample_batch in enumerate(train_data):\n",
        "    inp, out = sample_batch\n",
        "    batch = inp.shape[1]\n",
        "\n",
        "    p_in, orig = zeroNormTor(inp[0].reshape(batch*number,19,2).to(device))\n",
        "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
        "    p_out,_ = zeroNormTor(out[0].reshape(batch*number,30,2).to(device),30, orig)\n",
        "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
        "\n",
        "    sample = p_in.shape[1]\n",
        "    multiplier = p_out.shape[1]\n",
        "\n",
        "    p = torch.cat([p_in, p_out], dim=1)\n",
        "    v = torch.cat([v_in, v_out], dim=1)\n",
        "\n",
        "    p_list = []\n",
        "    for i in range(multiplier):\n",
        "      pred = model(p[:,i:i+sample], v[:,i:i+sample])\n",
        "\n",
        "      loss = 0\n",
        "      \n",
        "      p_loss = torch.sqrt(p_criteria(pred[0], p[:,i+sample]))\n",
        "      if(len(p_list)==0):\n",
        "        p_list = torch.unsqueeze(pred[0],dim=1)\n",
        "      else:\n",
        "        p_list = torch.cat([p_list, torch.unsqueeze(pred[0],dim=1)],dim=1)\n",
        "\n",
        "      v_loss = torch.sqrt(v_criteria(pred[1], v[:,i+sample]))\n",
        "\n",
        "      loss = p_loss + v_loss\n",
        "      my_optim.zero_grad()\n",
        "      loss.backward()\n",
        "      my_optim.step()\n",
        "\n",
        "    p_loss = torch.sqrt(p_criteria(p_list, p_out))\n",
        "    epoch_loss += p_loss\n",
        "\n",
        "  print(\"Training Loss: \", t, epoch_loss.item(), time.time() - start)\n",
        "  \n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  \n",
        "  for i_batch, sample_batch in enumerate(val_data):\n",
        "    inp, out = sample_batch\n",
        "    batch = inp.shape[1]\n",
        "\n",
        "    p_in, orig = zeroNormTor(inp[0].reshape(batch*number,19,2).to(device))\n",
        "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
        "    p_out,_ = zeroNormTor(out[0].reshape(batch*number,30,2).to(device),30, orig)\n",
        "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
        "\n",
        "    sample = p_in.shape[1]\n",
        "    multiplier = 30\n",
        "    p = p_in\n",
        "    v = v_in\n",
        "\n",
        "    for i in range(multiplier):\n",
        "      pred = model(p[:,i:i+sample], v[:,i:i+sample])\n",
        "\n",
        "      p = torch.cat([p, torch.unsqueeze(pred[0],dim=1)],dim=1)\n",
        "      v = torch.cat([v, torch.unsqueeze(pred[1],dim=1)],dim=1)\n",
        "\n",
        "    p_loss = torch.sqrt(p_criteria(p[:,19:], p_out))\n",
        "    epoch_loss +=p_loss.item()\n",
        "  print(\"Validation Loss:\", epoch_loss)\n",
        "  epoch_loss = 0\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtPAZa2azJ4r",
        "outputId": "55804b5b-82f7-465a-994f-1f1238341b6b"
      },
      "source": [
        "# train on multiple cars per\n",
        "import time\n",
        "\n",
        "batch_sz = 100\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "\n",
        "train_size = int(0.8 * len(train_loader.dataset))\n",
        "val_size = len(train_loader.dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
        "\n",
        "print(\"\\nLENGTH OF TRAIN LOADER DATASET:\", len(train_loader.dataset))\n",
        "print(\"LENGTH OF TRAIN DATA:\", len(train_data), \"\\nLENGTH OF VAL DATA:\", len(val_data))\n",
        "\n",
        "train_data = DataLoader(train_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "val_data = DataLoader(val_data, batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train_multiple, num_workers=2)\n",
        "model = Trajectory().to(device)\n",
        "\n",
        "my_optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# decayRate = 0.999    \n",
        "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
        "\n",
        "epoch = 30 # takes around 20 epochs to converge\n",
        "number = 5 # number of cars in each \n",
        "\n",
        "best_val = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  start = time.time()\n",
        "  \n",
        "  # train_loop = tqdm(enumerate(train_data), total=len(train_data))\n",
        "\n",
        "  for i_batch, sample_batch in enumerate(train_data):\n",
        "\n",
        "    inp, out = sample_batch\n",
        "    batch = inp.shape[1]\n",
        "\n",
        "    p_in, orig = zeroNormTor(inp[0].reshape(batch*number,19,2).to(device))\n",
        "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
        "    p_out,_ = zeroNormTor(out[0].reshape(batch*number,30,2).to(device),30, orig)\n",
        "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
        "\n",
        "    pred = model(p_in, v_in)\n",
        "\n",
        "\n",
        "    loss = 0\n",
        "    p_criteria = nn.MSELoss()\n",
        "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
        "\n",
        "    v_criteria = nn.MSELoss()\n",
        "    v_loss = torch.sqrt(v_criteria(pred[1], v_out))\n",
        "\n",
        "    loss = p_loss + v_loss\n",
        "    epoch_loss += p_loss.item()\n",
        "\n",
        "    my_optim.zero_grad()\n",
        "    loss.backward()\n",
        "    my_optim.step()\n",
        "\n",
        "    # ur tqdm stuff broke\n",
        "    # train_loop.set_description(f\"Train Epoch [{i + 1}/{epoch}]\")\n",
        "    # train_loop.set_postfix(loss = epoch_loss.item()) \n",
        "    # my_lr_scheduler.step()\n",
        "    # if(i>2999):\n",
        "    #   temp = torch.unsqueeze(orig,1)\n",
        "      # temp = torch.repeat_interleave(temp,30,1)\n",
        "      # print((pred[0]*500+temp)[0], (p_out*500+temp)[0])\n",
        "    # break\n",
        "  print(\"Training Loss: \", i, epoch_loss, time.time() - start)\n",
        "  \n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  # val_loop = tqdm(enumerate(val_data), total=len(val_data))\n",
        "  for i_batch, sample_batch in enumerate(val_data):\n",
        "    inp, out = sample_batch\n",
        "    batch = inp.shape[1]\n",
        "\n",
        "    p_in,orig = zeroNormTor(inp[0].reshape(batch*number,19,2).to(device))\n",
        "    v_in = inp[1].reshape(batch*number,19,2).to(device)/85.\n",
        "    p_out,_ = zeroNormTor(out[0].reshape(batch*number,30,2).to(device),30, orig)\n",
        "    v_out = out[1].reshape(batch*number,30,2).to(device)/85.\n",
        "\n",
        "    pred = model(p_in, v_in)\n",
        "\n",
        "    loss = 0\n",
        "    p_criteria = nn.MSELoss()\n",
        "    p_loss = torch.sqrt(p_criteria(pred[0], p_out))\n",
        "\n",
        "    epoch_loss += p_loss.item()\n",
        "\n",
        "    # val_loop.set_description(f\"Val.  Epoch [{i + 1}/{epoch}]\")\n",
        "    # val_loop.set_postfix(loss = epoch_loss.item())\n",
        "  if(epoch_loss< best_val):\n",
        "    print(\"saved\")\n",
        "    best_val = epoch_loss\n",
        "\n",
        "  print(\"Validation Loss: \", epoch_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LENGTH OF TRAIN LOADER DATASET: 205942\n",
            "LENGTH OF TRAIN DATA: 164753 \n",
            "LENGTH OF VAL DATA: 41189\n",
            "Training Loss:  0 35.65819874498993 472.5862205028534\n",
            "saved\n",
            "Validation Loss:  4.988086823374033\n",
            "Training Loss:  1 21.0319468062371 478.26616406440735\n",
            "Validation Loss:  5.286611597053707\n",
            "Training Loss:  2 20.45644376054406 477.5138130187988\n",
            "saved\n",
            "Validation Loss:  4.877381956204772\n",
            "Training Loss:  3 19.9245780473575 478.4371635913849\n",
            "saved\n",
            "Validation Loss:  4.77798888925463\n",
            "Training Loss:  4 19.679950556717813 478.3485782146454\n",
            "Validation Loss:  4.857072846964002\n",
            "Training Loss:  5 19.285880726762116 477.92885971069336\n",
            "saved\n",
            "Validation Loss:  4.761179870925844\n",
            "Training Loss:  6 18.9284421633929 478.04212760925293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjNAZOPL9cx2",
        "outputId": "94852648-afa6-4d0e-e36f-cc14460e9493"
      },
      "source": [
        "import time \n",
        "\n",
        "batch_sz = 300\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate_train, num_workers=2)\n",
        "\n",
        "model = Trajectory().to(device)\n",
        "\n",
        "my_optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# decayRate = 0.999\n",
        "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=decayRate)\n",
        "\n",
        "epoch = 20 # takes around 20 epochs to converge\n",
        "number = 1 # number of cars in each \n",
        "print(len(train_loader.dataset))\n",
        "\n",
        "p_criteria = nn.MSELoss()\n",
        "v_criteria = nn.MSELoss()\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  start = time.time()\n",
        "  for i_batch, sample_batch in enumerate(train_loader):\n",
        "    inp, out = sample_batch\n",
        "    batch = inp.shape[0]\n",
        "\n",
        "    p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
        "    v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
        "    p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
        "    v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
        "\n",
        "    sample = p_in.shape[1]\n",
        "    multiplier = p_out.shape[1]\n",
        "\n",
        "    p = torch.cat([p_in, p_out], dim=1)\n",
        "    v = torch.cat([v_in, v_out], dim=1)\n",
        "\n",
        "    p_list = []\n",
        "    for i in range(multiplier):\n",
        "      pred = model(p[:,i:i+sample], v[:,i:i+sample])\n",
        "\n",
        "      loss = 0\n",
        "      \n",
        "      p_loss = torch.sqrt(p_criteria(pred[0], p[:,i+sample]))\n",
        "      if(len(p_list)==0):\n",
        "        p_list = torch.unsqueeze(pred[0],dim=1)\n",
        "      else:\n",
        "        p_list = torch.cat([p_list, torch.unsqueeze(pred[0],dim=1)],dim=1)\n",
        "\n",
        "      v_loss = torch.sqrt(v_criteria(pred[1], v[:,i+sample]))\n",
        "\n",
        "      loss = p_loss + v_loss\n",
        "      my_optim.zero_grad()\n",
        "      loss.backward()\n",
        "      my_optim.step()\n",
        "\n",
        "\n",
        "    p_loss = torch.sqrt(p_criteria(p_list, p_out))\n",
        "    epoch_loss += p_loss\n",
        "\n",
        "  print(\"Training Loss: \", i, epoch_loss.item(), time.time() - start)\n",
        "  \n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "205942\n",
            "Training Loss:  29 1.9498718976974487 343.7735676765442\n",
            "Training Loss:  29 0.7784473299980164 347.1312220096588\n",
            "Training Loss:  29 0.5861884951591492 351.60469460487366\n",
            "Training Loss:  29 0.49096304178237915 348.19951248168945\n",
            "Training Loss:  29 0.4370681047439575 346.88306307792664\n",
            "Training Loss:  29 0.39462095499038696 348.0212461948395\n",
            "Training Loss:  29 0.3679283559322357 346.5364649295807\n",
            "Training Loss:  29 0.3424796164035797 346.18383526802063\n",
            "Training Loss:  29 0.3258410096168518 347.2324411869049\n",
            "Training Loss:  29 0.31043338775634766 346.28952169418335\n",
            "Training Loss:  29 0.29714950919151306 346.1159317493439\n",
            "Training Loss:  29 0.285498708486557 346.20251846313477\n",
            "Training Loss:  29 0.27799952030181885 347.88939690589905\n",
            "Training Loss:  29 0.2672721743583679 346.81219387054443\n",
            "Training Loss:  29 0.2621701955795288 347.6236653327942\n",
            "Training Loss:  29 0.25984466075897217 347.6486961841583\n",
            "Training Loss:  29 0.2521623969078064 346.34811091423035\n",
            "Training Loss:  29 0.244770348072052 347.7841649055481\n",
            "Training Loss:  29 0.23960435390472412 347.67326831817627\n",
            "Training Loss:  29 0.24183282256126404 346.50380420684814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uQ2Y3O0ELNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc80998-c226-4c0e-9581-4a7dc323ef23"
      },
      "source": [
        "model.eval()\n",
        "p_criteria = nn.MSELoss()\n",
        "\n",
        "for i_batch, sample_batch in enumerate(train_loader):\n",
        "  inp, out = sample_batch\n",
        "  batch = inp.shape[0]\n",
        "\n",
        "  p_in = (inp[:,0].reshape(batch*number,19,2).to(device))/4800. # -2400.)/2400.\n",
        "  v_in = inp[:,1].reshape(batch*number,19,2).to(device)/100.\n",
        "  p_out = (out[:,0].reshape(batch*number,30,2).to(device))/4800. # -2400.)/2400.\n",
        "  v_out = out[:,1].reshape(batch*number,30,2).to(device)/100.\n",
        "\n",
        "  sample = p_in.shape[1]\n",
        "  multiplier = 30\n",
        "  p = p_in\n",
        "  v = v_in\n",
        "\n",
        "  for i in range(multiplier):\n",
        "    pred = model(p[:,i:i+sample], v[:,i:i+sample])\n",
        "\n",
        "    p = torch.cat([p, torch.unsqueeze(pred[0],dim=1)],dim=1)\n",
        "    v = torch.cat([v, torch.unsqueeze(pred[1],dim=1)],dim=1)\n",
        "\n",
        "  p_loss = torch.sqrt(p_criteria(p[:,19:], p_out))\n",
        "  print(p_loss)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0054, device='cuda:0', grad_fn=<SqrtBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhL6nFx1q3bm"
      },
      "source": [
        "import csv \n",
        "model.eval()\n",
        "temp = []\n",
        "\n",
        "new_path = \"/content/new_val_in\"\n",
        "val_dataset  = ArgoverseDataset(data_path=new_path)\n",
        "\n",
        "top = []\n",
        "top.append(\"ID\")\n",
        "for i in range(60):\n",
        "  top.append(\"v\"+str(i+1))\n",
        "temp.append(top)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in val_dataset:\n",
        "      row = []\n",
        "      scene = i['scene_idx']\n",
        "      agent = i['agent_id']\n",
        "      target =0\n",
        "      for x in range(len(i['track_id'])):\n",
        "        if i['track_id'][x][0] == agent:\n",
        "          target = x\n",
        "\n",
        "      p_in = torch.LongTensor(i['p_in'])\n",
        "      v_in = torch.LongTensor(i['v_in'])\n",
        "\n",
        "      p_in = (p_in.reshape(60,19,2).to(device))/4800.\n",
        "      v_in = v_in.reshape(60,19,2).to(device)/100.\n",
        "\n",
        "      sample = p_in.shape[1]\n",
        "      multiplier = 30\n",
        "      p = p_in\n",
        "      v = v_in\n",
        "\n",
        "      for i in range(multiplier):\n",
        "        pred = model(p[:,i:i+sample], v[:,i:i+sample])\n",
        "\n",
        "        p = torch.cat([p, torch.unsqueeze(pred[0],dim=1)],dim=1)\n",
        "        v = torch.cat([v, torch.unsqueeze(pred[1],dim=1)],dim=1)\n",
        "\n",
        "      pred_out = p[:,19:]*4800.\n",
        "      \n",
        "      output = pred_out[target]\n",
        "\n",
        "      row.append(scene)\n",
        "      row = row + torch.flatten(output).cpu().detach().numpy().tolist()\n",
        "      temp.append(row)\n",
        "\n",
        "with open('submission3.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(temp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}